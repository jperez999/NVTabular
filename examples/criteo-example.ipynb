{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "from time import time\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "# tools for data preproc/loading\n",
    "import torch\n",
    "import rmm\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import Normalize,  Categorify,  LogOp, FillMissing, Clip, LambdaClassOp, get_embedding_sizes\n",
    "from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n",
    "from nvtabular.utils import device_mem_size\n",
    "import cudf\n",
    "\n",
    "# tools for training\n",
    "from fastai.basics import Learner\n",
    "from fastai.tabular.model import TabularModel\n",
    "from fastai.tabular.data import TabularDataLoaders\n",
    "from fastai.metrics import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some information about where to get our data\n",
    "INPUT_DATA_DIR = os.environ.get('INPUT_DATA_DIR', '/raid/criteo/tests/crit_int_pq')\n",
    "OUTPUT_DATA_DIR = os.environ.get('OUTPUT_DATA_DIR', '/raid/criteo/tests/newmu') # where we'll save our procesed data to\n",
    "BATCH_SIZE = int(os.environ.get('BATCH_SIZE', 32768))\n",
    "PARTS_PER_CHUNK = int(os.environ.get('PARTS_PER_CHUNK', 10))\n",
    "SHUFFLE = True\n",
    "NUM_TRAIN_DAYS = 23 # number of days worth of data to use for training, the rest will be used for validation\n",
    "\n",
    "# define our dataset schema\n",
    "CONTINUOUS_COLUMNS = ['I' + str(x) for x in range(1,14)]\n",
    "CATEGORICAL_COLUMNS =  ['C' + str(x) for x in range(1,27)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "COLUMNS = CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS + LABEL_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'day_{}.parquet'\n",
    "num_days = len([i for i in os.listdir(INPUT_DATA_DIR) if re.match(fname.format('[0-9]{1,2}'), i) is not None])\n",
    "train_paths = [os.path.join(INPUT_DATA_DIR, fname.format(day)) for day in range(NUM_TRAIN_DAYS)][:1]\n",
    "valid_paths = [os.path.join(INPUT_DATA_DIR, fname.format(day)) for day in range(NUM_TRAIN_DAYS, num_days)][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WritePartChunk:\n",
    "    \"\"\"\n",
    "    Write one partition before it has been collated into a chunk.\n",
    "    \"\"\"\n",
    "    def _exec(self, gdf):\n",
    "        \"\"\"\n",
    "        Ensure only one parition represented.\n",
    "        \"\"\"\n",
    "        self.f_name = open(\"./rando/part_chunk.txt\", \"a+\")\n",
    "        if gdf[\"part_idx\"].min() == gdf[\"part_idx\"].max():\n",
    "            part_idx = gdf[\"part_idx\"][0]\n",
    "            self.f_name.write(f\"{part_idx} {gdf.shape[0]} \\n\")\n",
    "        return gdf\n",
    "    def __del__(self):\n",
    "        self.f_name.close()\n",
    "        \n",
    "class WritePart:\n",
    "    \"\"\"\n",
    "    Write one chunk at a time, consisting of multiple partitions\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dir, col_name=\"part_idx\", group=\"batch\", extra_cols=None, export=True):\n",
    "        self.count = 0\n",
    "        self.col_name = col_name\n",
    "        self.group = group\n",
    "        self.in_dir = in_dir\n",
    "        self.export = export\n",
    "        self.extra_cols = extra_cols\n",
    "    \n",
    "    def _exec(self, gdf, columns_ctx):\n",
    "        gdf[self.col_name] = self.count\n",
    "        if 'label' not in columns_ctx[\"final\"][\"ctx\"]:\n",
    "            columns_ctx[\"final\"][\"ctx\"]['label'] = []\n",
    "        if self.col_name not in columns_ctx[\"final\"][\"ctx\"]['label']:\n",
    "            columns_ctx[\"final\"][\"ctx\"]['label'].append(self.col_name)\n",
    "        self.count = self.count + 1\n",
    "        return gdf\n",
    "\n",
    "class WriteCollectParts:\n",
    "    def __init__(self, in_dir, col_name=\"part_idx\", group=\"batch\", export=True, extra_cols = []):\n",
    "        self.count = 0\n",
    "        self.col_name = col_name\n",
    "        self.group = group\n",
    "        self.in_dir = in_dir\n",
    "        self.export = export\n",
    "        self.extra_cols = extra_cols\n",
    "    \n",
    "    def _exec(self, gdf):\n",
    "        gdf[self.col_name] = self.count\n",
    "        if self.export:\n",
    "            cols = self.extra_cols.append(self.col_name)\n",
    "            file_name = f\"{self.group}_{self.count}.parquet\"\n",
    "            file_path = os.path.join(self.in_dir, file_name)\n",
    "            gdf[cols].to_parquet(file_path)\n",
    "        self.count = self.count + 1\n",
    "        return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = {}\n",
    "bb = WritePart(\"/raid/criteo/tests/dump\", col_name=\"part_origin\", group=\"origin\", export=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = nvt.Workflow(\n",
    "    cat_names=CATEGORICAL_COLUMNS,\n",
    "    cont_names=CONTINUOUS_COLUMNS,\n",
    "    label_name=LABEL_COLUMNS)\n",
    "\n",
    "# log -> normalize continuous features. Note that doing this in the opposite\n",
    "# order wouldn't make sense! Note also that we're zero filling continuous\n",
    "# values before the log: this is a good time to remember that LogOp\n",
    "# performs log(1+x), not log(x)\n",
    "proc.add_feature(LambdaClassOp(bb))\n",
    "proc.add_cont_feature([FillMissing(), Clip(min_value=0), LogOp()])\n",
    "proc.add_cont_preprocess(Normalize())\n",
    "\n",
    "# categorification with frequency thresholding\n",
    "proc.add_cat_preprocess(Categorify(freq_threshold=15, columns=CATEGORICAL_COLUMNS, out_path=OUTPUT_DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_dir = os.path.join(OUTPUT_DATA_DIR, 'train/')\n",
    "output_valid_dir = os.path.join(OUTPUT_DATA_DIR, 'valid/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = nvt.Dataset(train_paths, engine='parquet', part_mem_fraction=0.15, callbacks=cb)\n",
    "valid_dataset = nvt.Dataset(valid_paths, engine='parquet', part_mem_fraction=0.15, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.1 s, sys: 43.6 s, total: 1min 27s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "proc.apply(train_dataset, shuffle=nvt.io.Shuffle.PER_PARTITION, output_path=output_train_dir, out_files_per_proc=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.6 s, sys: 31.6 s, total: 57.2 s\n",
      "Wall time: 58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "proc.apply(valid_dataset, record_stats=False, shuffle=nvt.io.Shuffle.PER_PARTITION, output_path=output_valid_dir, out_files_per_proc=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_dir = os.path.join(OUTPUT_DATA_DIR, 'train/')\n",
    "output_valid_dir = os.path.join(OUTPUT_DATA_DIR, 'valid/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmm.reinitialize(pool_allocator=True, initial_pool_size=int(0.3 * device_mem_size(kind='free')/256) * 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = glob.glob(os.path.join(output_train_dir, \"*.parquet\"))\n",
    "valid_paths = glob.glob(os.path.join(output_valid_dir, \"*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = nvt.Dataset(train_paths, engine=\"parquet\", part_mem_fraction=0.04/PARTS_PER_CHUNK)\n",
    "valid_data = nvt.Dataset(valid_paths, engine=\"parquet\", part_mem_fraction=0.04/PARTS_PER_CHUNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WritePartChunk:\n",
    "    \"\"\"\n",
    "    Write one partition before it has been collated into a chunk.\n",
    "    \"\"\"\n",
    "    def _exec(self, gdf):\n",
    "        \"\"\"\n",
    "        Ensure only one parition represented.\n",
    "        \"\"\"\n",
    "        self.f_name = open(\"./rando/part_chunk.txt\", \"a+\")\n",
    "        if gdf[\"part_idx\"].min() == gdf[\"part_idx\"].max():\n",
    "            part_idx = gdf[\"part_idx\"][0]\n",
    "            self.f_name.write(f\"{part_idx} {gdf.shape[0]} \\n\")\n",
    "        return gdf\n",
    "    def __del__(self):\n",
    "        self.f_name.close()\n",
    "        \n",
    "\n",
    "callbacks = {}\n",
    "callbacks[\"PART_CHUNK\"] = [WritePartChunk()]\n",
    "callbacks[\"BATCH_GET\"] = [WriteCollectParts(\"/raid/criteo/tests/dump\", extra_cols=[\"part_origin\"])]\n",
    "\n",
    "\n",
    "train_data_itrs = TorchAsyncItr(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    cats=CATEGORICAL_COLUMNS,\n",
    "    conts=CONTINUOUS_COLUMNS,\n",
    "    labels=LABEL_COLUMNS,\n",
    "    parts_per_chunk=PARTS_PER_CHUNK,\n",
    "    callbacks=callbacks,\n",
    "    shuffle=SHUFFLE,\n",
    ")\n",
    "valid_data_itrs = TorchAsyncItr(\n",
    "    valid_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    cats=CATEGORICAL_COLUMNS,\n",
    "    conts=CONTINUOUS_COLUMNS,\n",
    "    labels=LABEL_COLUMNS,\n",
    "    parts_per_chunk=PARTS_PER_CHUNK,\n",
    "    callbacks=callbacks,\n",
    "    shuffle=SHUFFLE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_col(batch):\n",
    "    return (batch[0], batch[1], batch[2].long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DLDataLoader(train_data_itrs, collate_fn=gen_col, batch_size=None, pin_memory=False, num_workers=0)\n",
    "valid_dataloader = DLDataLoader(valid_data_itrs, collate_fn=gen_col, batch_size=None, pin_memory=False, num_workers=0)\n",
    "databunch = TabularDataLoaders(train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [(7599500, 16),\n",
    " (5345303, 16),\n",
    " (561810, 16),\n",
    " (242827, 16),\n",
    " (11, 6),\n",
    " (2209, 16),\n",
    " (10616, 16),\n",
    " (100, 16),\n",
    " (4, 3),\n",
    " (968, 16),\n",
    " (15, 7),\n",
    " (33521, 16),\n",
    " (7838519, 16),\n",
    " (2580502, 16),\n",
    " (6878028, 16),\n",
    " (298771, 16),\n",
    " (11951, 16),\n",
    " (97, 16),\n",
    " (35, 12),\n",
    " (17022, 16),\n",
    " (7339, 16),\n",
    " (20046, 16),\n",
    " (4, 3),\n",
    " (7068, 16),\n",
    " (1377, 16),\n",
    " (63, 16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TabularModel(emb_szs=embeddings, n_cont=len(CONTINUOUS_COLUMNS), out_sz=2, layers=[512, 256]).cuda()\n",
    "learn =  Learner(databunch, model, loss_func = torch.nn.CrossEntropyLoss(), metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callback.schedule import fit_one_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.32e-2\n",
    "epochs = 1\n",
    "start = time()\n",
    "#learn.fit(epochs, learning_rate)\n",
    "fit_one_cycle(learn, n_epoch=epochs, lr_max=learning_rate)\n",
    "t_final = time() - start\n",
    "print(t_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
